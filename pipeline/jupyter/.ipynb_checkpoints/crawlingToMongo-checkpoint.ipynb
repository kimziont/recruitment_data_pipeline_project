{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b5ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import time\n",
    "import webbrowser\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a3dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "path = '/usr/local/lib/python3.8/site-packages/chromedriver/usr/bin/chromedriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e00821",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb://root:root@mongodb:27017')\n",
    "db = client[\"recruitment\"]\n",
    "collection = db[\"programmers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff38dd03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37/1866586606.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=path, chrome_options=chrome_options)\n",
      "/tmp/ipykernel_37/1866586606.py:1: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(executable_path=path, chrome_options=chrome_options)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(executable_path=path, chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1372b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "-----1번 째 페이지 크롤링중----------------------------\n",
      "{'마크애니_[부산지역] 선별관제 솔루션 필드 엔지니어 부문', '프렌트립_데이터 엔지니어(Data Engineer)', '잉클_산업용 설비 데이터 분석가', '스페이스워크_Data Scientist', '빅픽처팀_데이터 엔지니어', '스페이스워크_ML Engineering (강화학습 기반 건축설계 자동화 엔진 개발)', '케어마인드_AI 엔지니어', '메쉬코리아_데이터 엔지니어', '어센트코리아_빅데이터플랫폼개발자', '인포보스_[인포보스]인공지능개발팀(산업기능요원, 전문연구요원 가능)', '인젠트_클라우드 사업실(마이데이터, APIM) 개발자', '해줌_[에너지 IT기업 해줌] 백엔드 개발자', '룩코_데이터 크롤링 및 시스템 구축 엔지니어', '웨이브릿지_[웨이브릿지] DW Engineer', '스테이지파이브_데이터 엔지니어', '페이타랩_[패스오더] 백엔드(Spring, Python, MSA)개발자 (병역특례 산업기능요원 보충역/전직가능)', '메쉬코리아_ML Engineer', '위메이드_Data Engineer', 'AB180_Data Scientist (데이터 사이언티스트)(병역특례 가능)', '페이타랩_[패스오더] 데이터 엔지니어(그로스해커)'}\n"
     ]
    }
   ],
   "source": [
    "driver.get('https://programmers.co.kr/job')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "crawling_set = set()\n",
    "for i in range(8, 9):\n",
    "    \n",
    "    # 직무 클릭 \n",
    "    job_category_button = driver.find_element(by=By.XPATH,value=\"/html/body/div[2]/div/section[1]/div/div[2]/div[1]/button\")\n",
    "    job_category_button.click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # 포지션 선택 /html/body/div[2]/div/section[1]/div/div[2]/div[1]/div/ul/li[8]/label\n",
    "    i_th_job_category_xpath = f'/html/body/div[2]/div/section[1]/div/div[2]/div[1]/div/ul/li[{i}]/label'\n",
    "    i_th_job_category = driver.find_element(by=By.XPATH, value=i_th_job_category_xpath)\n",
    "    i_th_job_category.click()\n",
    "    time.sleep(1)\n",
    "    position_text = i_th_job_category.text\n",
    "    \n",
    "    # 경력 클릭\n",
    "    career_xpath = '/html/body/div[2]/div/section[1]/div/div[3]/div[1]/div[1]/button'\n",
    "    driver.find_element(by=By.XPATH, value=career_xpath).click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # 신입 선택\n",
    "    newbie_xpath = '/html/body/div[2]/div/section[1]/div/div[3]/div[1]/div[1]/div/div/div/div[2]/label'\n",
    "    driver.find_element(by=By.XPATH, value=newbie_xpath).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 페이지 돈다\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    \n",
    "    page = 1\n",
    "    last_page_num = int(soup.find_all(class_='page-link')[-2].text.strip())\n",
    "    \n",
    "    \n",
    "\n",
    "    print(last_page_num)\n",
    "    for cur_page in range(last_page_num):\n",
    "        print(f'-----{cur_page + 1}번 째 페이지 크롤링중----------------------------')\n",
    "        # 현재 페이지 저장해두기\n",
    "        prev_url = driver.current_url\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "\n",
    "        position_list = [p.text.strip() for p in soup.find_all(class_='position-link')]\n",
    "        company_list = [c.text.strip() for c in soup.find_all(class_='company-link')]\n",
    "        company_list = list(filter(lambda x: len(x), company_list))\n",
    "        \n",
    "        post_id_list = [c + \"_\" + p for c,p in zip(company_list, position_list)]\n",
    "        \n",
    "        for post_id in  post_id_list:\n",
    "            crawling_set.add(post_id)\n",
    "\n",
    "        \n",
    "        # 아래 회사 리스트 for문 돌면서 크롤링 len(position_list) + 1\n",
    "        for j in range(1, len(position_list) + 1):\n",
    "            row_data = dict()\n",
    "            row_data['Category'] = position_text\n",
    "            try:\n",
    "                j_th_position_xpath = f'/html/body/div[2]/div/section[2]/div/ul/li[{j}]/div[2]/div[1]/h5/a'\n",
    "                driver.find_element(by=By.XPATH, value=j_th_position_xpath).click()\n",
    "            except:\n",
    "                print(f\"{page}페이지, {j}번째 글을 읽어오지 못했습니다\")\n",
    "                continue\n",
    "            time.sleep(2)\n",
    "\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            if title := soup.select_one(\".position-show\").select_one(\".title\").text.strip():\n",
    "                row_data['Title'] = title\n",
    "            \n",
    "            if company := soup.select_one(\".position-show\").select_one(\".sub-title\").text.strip():\n",
    "                row_data['Company'] = company\n",
    "            \n",
    "            \n",
    "            \n",
    "            row_data['_id'] = row_data['Company'] + '_' + row_data['Title']\n",
    "        \n",
    "            if summary := soup.select_one(\".position-show\").select_one(\".section-summary\"):\n",
    "                label_list = summary.find_all(class_='t-label')\n",
    "                value_list = summary.find_all(class_='t-content')\n",
    "                for label, value in zip(label_list, value_list):\n",
    "                    if label.text.strip() == \"사원수\":\n",
    "                        row_data['EmployeeNumber'] = value.text.strip()[:-1].strip()\n",
    "                    else:\n",
    "                        label = label.text.split(\" \")\n",
    "                        label = \"\".join(list(map(lambda x: x.capitalize(), label))).strip()\n",
    "                        row_data[label] = value.text.strip()\n",
    "        \n",
    "            if stacks := soup.select_one(\".position-show\").select_one(\".section-stacks\"):\n",
    "                stack_list = stacks.find_all(class_='_3w5q5pj0-cKLdyuMSZY5ES _1rWv2ZoV6KRiA6VmGgEqZd')\n",
    "                row_data['Stacks'] = \",\".join([stack.get_text(strip=True).replace(\"\\xa0\", \"\") for stack in stack_list])\n",
    "            \n",
    "            if description:= soup.select_one(\".content-body.col-item.col-xs-12.col-sm-12.col-md-12.col-lg-8\")\\\n",
    "                                            .select_one(\".section-position\")\\\n",
    "                                            .select_one(\".markdown.github.markdown-viewer\").text:\n",
    "\n",
    "                d = re.sub(r'[^\\w\\d\\n,.!/()[\\]{} ]', \"\", description)\n",
    "                d = re.split(r'[\\n]', d)\n",
    "\n",
    "                row_data['Description'] = list(filter(lambda x: len(x), d))\n",
    "            \n",
    "            if requirements := soup.select_one(\".content-body.col-item.col-xs-12.col-sm-12.col-md-12.col-lg-8\")\\\n",
    "                                .select_one(\".section-requirements\")\\\n",
    "                                .select_one(\".markdown.github.markdown-viewer\").text:\n",
    "\n",
    "                r = re.sub(r'[^\\w\\d\\n,.!/()[\\]{} ]', \"\", requirements)\n",
    "                r = re.split(r'[\\n]', r)\n",
    "\n",
    "                row_data['Requirements'] = list(filter(lambda x: len(x), r))\n",
    "\n",
    "            if preference := soup.select_one(\".content-body.col-item.col-xs-12.col-sm-12.col-md-12.col-lg-8\")\\\n",
    "                                            .select_one(\".section-preference\")\\\n",
    "                                            .select_one(\".markdown.github.markdown-viewer\").text:\n",
    "\n",
    "                p = re.sub(r'[^\\w\\d\\n,.!/()[\\]{} ]', \"\", preference)\n",
    "                p = re.split(r'[\\n]', p)\n",
    "\n",
    "                row_data['Preference'] = list(filter(lambda x: len(x), p))\n",
    "                \n",
    "                row_data['Status'] = 'In progress'\n",
    "            \n",
    "            # Insert\n",
    "            if not collection.find_one({'_id': row_data['_id']}):\n",
    "                row_data['CreatedTime'] = datetime.now(timezone('Asia/Seoul')).strftime(\"%y/%m/%d %H:%M:%S\")\n",
    "                row_data['LastModifiedTime'] = row_data['CreatedTime']\n",
    "                collection.insert_one(row_data)\n",
    "            # Replace (값의 일부(Requirements)를 확인하여 변경사항 있다면 아예 새로운 데이터로 바꾼다\n",
    "            # Update 쓰면 $set 이런식으로 해줘야 해서 불편하다\n",
    "            else:\n",
    "                db_data = collection.find_one({'_id': row_data['_id']})\n",
    "                if row_data['Requirements'] != db_data['Requirements']:\n",
    "                    row_data['CreatedTime'] = db_data['CreatedTime']\n",
    "                    row_data['LastModifiedTime'] = datetime.now(timezone('Asia/Seoul')).strftime(\"%y/%m/%d %H:%M:%S\")\n",
    "                    collection.replace_one({'_id': row_data['_id']}, row_data)\n",
    "            \n",
    "            \n",
    "            df = df.append(row_data, ignore_index=True)\n",
    "            \n",
    "            # 뒤로가기\n",
    "            driver.get(prev_url)\n",
    "            time.sleep(1)\n",
    "            \n",
    "        \n",
    "        if last_page_num >= 7:\n",
    "            pagination = 9\n",
    "        else:\n",
    "            pagination = last_page_num + 2\n",
    "\n",
    "        if next_page := driver.find_element(by=By.XPATH, value=f'/html/body/div[2]/div/div[3]/ul/li[{pagination}]/span'):\n",
    "            page += 1\n",
    "            try:\n",
    "                next_page.click()\n",
    "                print(f'{page}페이지로 이동합니다')\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                break\n",
    "        else:\n",
    "            print(\"다음 페이지가 없습니다\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1fda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마갑된 채용 공고 Status 변경하기 (In progree -> Closed)\n",
    "for doc in collection.find():\n",
    "    if doc['_id'] not in crawling_set:\n",
    "        collection.find_one_and_update({'_id': doc['_id']}, {'$set' :{'Status': 'Closed'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951c98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
